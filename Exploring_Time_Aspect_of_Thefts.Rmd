---
title: "When To Leave Your House (aka Exploring Time Trends in Louisville Crime Data)"
output: html_document
---

```{r package_load, echo = FALSE, cache = TRUE, warning = FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(readr)
library(printr)
library(gridExtra)
library(grid)
options(scipen = 999)
```

```{r raw_data, echo = FALSE, cache = TRUE, warning = FALSE, message = FALSE}
raw_data <- read_csv("crime_lou_with_geocoding.csv")

create_date_variables <- function(df){
  require(lubridate)
  # Uses the POSIXct date_occured variable to create useful date related
  # subvariables
  df$year <- year(df$date_occured)
  df$month <- month(df$date_occured)
  df$day <- day(df$date_occured)
  df$hour <- hour(df$date_occured)
  df$year_month <- paste(df$year, df$month, sep = '-')
  df$day_of_week <- wday(df$date_occured, label = TRUE, abbr = FALSE)
  df$weekday <- ifelse(df$day_of_week == "Saturday" | df$day_of_week == "Sunday", 
                              "Weekend", "Weekday")
  df$yday <- yday(df$date_occured)
  df$date <- as.Date(df$date_occured)
  df$week <- week(df$date_occured)
  
  return(df)
}

# Temporary alteration of date variation function- made to use reported date instead of occured date
create_date_variables2 <- function(df){
  require(lubridate)
  # Uses the POSIXct date_occured variable to create useful date related
  # subvariables
  df$year <- year(df$date_reported)
  df$month <- month(df$date_reported)
  df$day <- day(df$date_reported)
  df$hour <- hour(df$date_reported)
  df$year_month <- paste(df$year, df$month, sep = '-')
  df$day_of_week <- wday(df$date_reported, label = TRUE, abbr = FALSE)
  df$weekday <- ifelse(df$day_of_week == "Saturday" | df$day_of_week == "Sunday", 
                              "Weekend", "Weekday")
  df$yday <- yday(df$date_reported)
  df$date <- as.Date(df$date_reported)
  
  return(df)
}
#create season variable based on 2016 season dates
getSeason <- function(dates){
  winter <- as.Date("2016-12-21", format = "%Y-%m-%d")
  spring <- as.Date("2016-3-20", format = "%Y-%m-%d")
  summer <- as.Date("2016-6-20", format = "%Y-%m-%d")
  fall <- as.Date("2016-9-22", format = "%Y-%m-%d")
  
  d <- as.Date(strftime(dates, format = "2016-%m-%d"))
  
  ifelse(d >= winter | d < spring, "Winter",
         ifelse(d >= spring & d < summer, "Spring",
                ifelse(d >= summer & d < fall, "Summer", "Fall")))
}

crime_lou <- create_date_variables(raw_data)
crime_lou$season <- getSeason(crime_lou$date)

# Scrape the official list of Louisville zip codes (does not include 
# the 'nonstandard' zip codes i.e. PO Box or business zip codes)
library(rvest)
html <- read_html("http://kentucky.hometownlocator.com/zip-codes/zipcodes,city,louisville.cfm")
zips <- html_nodes(html, "tr:nth-child(3) a")
zips <- html_text(zips)

# Filtering the crimes to only include official, standard, USPS zip_codes for Louisville.
# This gets rid of about 43000 records that seem to have mislabeled or unusual 
# zip code data. Also went ahead and filtered to the 2005-2015 data -- while
# earlier data exists, it is sparse.
crime_lou <- crime_lou%>%
  filter(zip_code %in% zips & year >= 2005 & year <= 2015)
```
# Introduction
So far in this series, I have been concentrating more on the spatial components of specific 
crime categories.  Today, I would like to begin exploring the temporal aspect of the 
dataset. It is my impression that general police work assumes a certain temporal pattern
to criminal offenses. For instance, it is common to hear how crime is higher during the summer
months or how crime rates rise at night. But, not being well versed in criminal procedures,
I was curious whether this common knowledge was accurate or merely an outdate urban legend of sorts.
With that in mind, I will begin by examining how crime rates vary depending on the time variable
we are dealing with. Hopefully, this will yield some insight into the ebb and flow of crime
within Louisville and allow for some simple forecasting of future crime counts.


## Overview of Crime Variation by Various Time Elements
To begin, I want to quickly get a basic understanding of the fluctuation of crime in Louisville.
The plot below is a quick overview of crime counts when grouped by the day of the week, month,
hour, and day. These plots do mask yearly variation since they are aggregated, but it is
a good starting point.

```{r dow_percentage_calc, echo = FALSE, cache = TRUE, warning = FALSE, message = FALSE}
# Quick, dirty percentage calculations for use in the text below
by_dow <- crime_lou%>%
  group_by(day_of_week)%>%
  summarise(count = n())%>%
  mutate(perc_below_max = round(((max(count)/count)-1)*100, 2))

by_month <- crime_lou%>%
  group_by(month)%>%
  summarise(count = n())%>%
  mutate(perc_below_max = round(((max(count)/count)-1)*100, 2))

by_hour <- crime_lou%>%
  group_by(hour)%>%
  summarise(count = n())

n <- length(by_hour$count)
midnight_peak <- round(((max(by_hour$count)/sort(by_hour$count, partial = n-1)[n-1])-1) *100,2)

by_day <- crime_lou%>%
  group_by(day)%>%
  summarise(count = n())

d <- length(by_day$count)
first_day_peak <- round(((max(by_day$count)/sort(by_day$count, partial = d-1)[d-1])-1) *100,2)
last_day_valley <- -round(((min(by_day$count)/sort(by_day$count, partial = d-1)[d-1])-1) *100,2)
```
As you can see, there is a gradual increase in crime throughout the week up to Friday
(from Sunday to Friday we see a `r by_dow$perc_below_max[by_dow$day_of_week == 'Sunday']`% increase),
then a dropoff on the weekend. This is a bit counterintuitive to me, as I expected higher crime
levels on the weekend when people had less to occupy their time.  But that goes to show you
why you always need to look at the data first.

The monthly figures are about as I expected with a peak during high summer in July and August and
a gradual falling off on either side. We see a `r by_month$perc_below_max[by_month$month == '2']`%
rise from the minimum in February to the max in August. January does seem to be oddly high, but 
I believe this is related to record keeping procedures we will delve into in a moment.

In the hourly plot, two things are immediately apparent.  One, there is a definite hourly trend present.
Basically, more crimes are committed during the time when most people are awake.  This makes a lot of sense,
but is not particularly insightful.  More interesting is the odd peak at 0 (midnight).
It might make sense that certain crimes are more prevalent during the late hours--for instance dui--
but a) this peak seems well above what you would expect for a couple crimes being more popular at night
and more importantly b) the surrounding hours don't show any signs of higher activity levels.
In fact, the midnight count is `r midnight_peak`% higher than the 'secondary' peak at hour
16. 

It is illustrative to examine the daily plot before exploring in depth what is going on with the hourly 
counts.  Two data points jump out when viewing this plot--the first and last bars.  Similar to the 
hourly plot, the first day of the month has a peak well above (`r first_day_peak`% above the next highest day)
what seems typical for the rest of the month. The last day of the month is basically the other side
of the coin--it is `r last_day_valley`% below the (non-first day of the month) peak. These 
two days stand out even more because of how consistent the rest of the daily counts
seem to be. If we were to discard the first and last days of the month, we would have 
very little evidence of any periodic effect.

```{r hdwm_full_dataset, echo = FALSE, cache = TRUE, out.width = '1100px', warning = FALSE, message = FALSE}
library(grid)
bar.theme <- 
  theme(legend.position = "none")+
  theme(plot.title = element_text(size = 14, family = "serif", face = 'bold', hjust = 0, color = "#666666"))+
  theme(axis.title = element_text(size = 12, family = "serif", face = 'bold', hjust = 0, color = "#666666"))+
  theme(axis.title.y = element_text(angle = 90))+
  theme(axis.title.x = element_text(hjust = 0, vjust = 0.5))+
  theme(axis.text.x = element_text(size = 5))+
  theme_bw()

# create hourly, daily, dow and monthly count plots. These are being used to illustrate
# various time periods present in the data
hourly <- crime_lou%>%
  group_by(hour)%>%
  summarise(count = n())%>%
  ggplot(aes(x = hour, y = count))+
  geom_bar(stat = 'identity', fill = 'darkblue', alpha = .4)+
  stat_smooth(method = 'auto', size = .6, alpha = .4, fill = "grey", colour = "red2")+
  scale_x_discrete(limits = seq(0, 23, by = 1))+
  labs(x = "Hour", y = "Count")+
  bar.theme

daily <- crime_lou%>%
  group_by(day)%>%
  summarise(count = n())%>%
  ggplot(aes(x = day, y = count))+
  geom_bar(stat = 'identity', fill = 'darkblue', alpha = .4)+
  stat_smooth(method = 'auto', size = .6, alpha = .4, fill = "grey", colour = "red2")+
  scale_x_discrete(limits = seq(1, 31, by = 1))+
  labs(x = "Day", y = "")+
  bar.theme

dayOfWeek <- crime_lou%>%
  group_by(day_of_week)%>%
  summarise(count = n())%>%
  ggplot(aes(x = day_of_week, y = count))+
  geom_bar(stat = 'identity', fill = 'darkblue', alpha = .4)+
  stat_smooth(method = 'auto', size = .6, alpha = .4, fill = "grey", colour = "red2")+
  labs(x = "Day of Week", y = "Count")+
  bar.theme

monthly <- crime_lou%>%
  group_by(month)%>%
  summarise(count = n())%>%
  ggplot(aes(x = month, y = count))+
  geom_bar(stat = 'identity', fill = 'darkblue', alpha = .4)+
  stat_smooth(method = 'auto', size = .6, alpha = .4, fill = "grey", colour = "red2")+
  scale_x_discrete(limits = month.abb)+
  labs(x = "Month", y = "")+
  bar.theme

# arranges the four plots into one image and formats it nicely
grid.arrange(dayOfWeek, monthly, hourly, daily, ncol = 2, top = textGrob("Crime Counts by", hjust = 0, x = unit(3.5, 'lines'), 
                                                                      gp = gpar(fontsize = 14,
                                                                      font = 2,
                                                                      fontfamily = 'serif',
                                                                      col = "#666666")))


```

After some thought, the last day of the month valley seems like it could be one of two 
things (and most probably a combination of both). One possibility is the varying number of days in a month results in skewed counts 
for the last couple days (particularly the 30th and 31st). In other words, since not 
every month has 31 days (or 30), the 31st day is underrepresented in the aggregated count.
This makes sense, and is something we can adjust for if we need to.

The second possibility is that this drop is a feature of how reports are handled. By that 
I mean that crime statistics are probably generated and reported monthly. If that is true,
then it might be in the departments best interest to roll crimes over to the next month, effectively
reducing the months crime rates. Of course, this probably ties into why the first of 
month has a spike of crime. In fact, if we take a single year and look at the data by month
and day, we can see this cycle. Although a time series plot is probably more appropriate,
a bar plot illustrates the cycle a little better here.

```{r, 2015_month_x_day, echo = FALSE, cache = TRUE,out.width = '1100px', warning = FALSE, message = FALSE}
crime_lou%>%
  filter(year == 2015)%>%
  group_by(month, day)%>%
  summarise(count = n())%>%
  ggplot(aes(x = day, y = count))+
  geom_bar(stat = 'identity', fill = 'darkblue', alpha = .4)+
  facet_wrap(~month)+
  stat_smooth(method = 'auto', size = .6, alpha = .4, fill = 'grey', colour = 'red2')+
  scale_x_discrete(limits = seq(1, 31, by = 2))+
  labs(x = "Day", y = "")+
  ggtitle("Daily Crime Counts, by Month")+
  bar.theme+
  theme(axis.text.x = element_text(size = 5))+
  theme(strip.background = element_rect(fill = 'white'),
        strip.text.x = element_text(size = 6))

fifteen <- crime_lou%>%
  filter(year == 2015)%>%
  group_by(date)%>%
  summarise(count = n())
# plot.ts(fifteen$count)

```

So you can see that the first day of the month tends to be one of the highest crime
days of the month and the last day of the month one of the lowest. In January, we actually
see an enormous spike, which, given what we have seen so far, tells me that there is probably
a yearly 'rollover' of crimes as well. 

While this all seems pretty logical, I want something more quantifiable to illustrate that
there is reporting bias. Since our original dataset has both 'date reported' and 'date occured'
variables, why don't I calculate the 'reporting gap' between when the crime occured and
when it was reported and then use that to see if these spikes are the result of 'old'
crimes finally being reported.

```{r reporting_gap_calculations, echo = FALSE, cache = TRUE, out.width = '1100px', warning = FALSE, message = FALSE}
# Calculate gap between date occured and date reported - rounded to nearest day
crime_lou$reporting_gap <- round(as.numeric(crime_lou$date_reported - crime_lou$date_occured, units = 'days'))

# Group the gaps into negatives (which don't make much sense), 0 or 1 day, 1 week, 2 weeks, 
# 1 month, 1 year and greater than 1 year
crime_lou$reporting <- cut(as.numeric(crime_lou$reporting_gap), breaks = c(-100000, 0, 2, 8, 15, 31, 365, 4073),
                           right = FALSE, include.lowest = TRUE)

levels(crime_lou$reporting) <- list(Neg = '[-1e+05,0)', "< 2 Days" = '[0,2)', '< 1 Week' = '[2,8)',
                                    '< 2 Weeks' = '[8,15)', '< 1 Month' = '[15,31)', '< 1 Year' = '[31,365)',
                                    '> 1 Year' = '[365,4.07e+03]')

crime_lou%>%
  group_by(hour, reporting)%>%
  summarise(count = n())%>%
  ggplot(aes(x = reporting, y = count))+
  geom_bar(stat = 'identity', fill = 'darkblue', alpha = .4)+
  facet_wrap(~hour)+
  stat_smooth(method = 'auto', size = .6, alpha = .4, fill = 'grey', colour = 'red2')+
  ggtitle("Length of Reporting Gap, by Hour")+
  labs(x = "", y = "")+
  bar.theme+
  theme(axis.text.x = element_text(size = 6, angle = 90, vjust = 0))+
  theme(axis.text.y = element_text(size = 7))+
  theme(strip.background = element_rect(fill = 'white'),
        strip.text.x = element_text(size = 6))

crime_lou%>%
  group_by(day, reporting)%>%
  summarise(count = n())%>%
  ggplot(aes(x = reporting, y = count))+
  geom_bar(stat = 'identity', fill = 'darkblue', alpha = .4)+
  facet_wrap(~day)+
  stat_smooth(method = 'auto', size = .6, alpha = .4, fill = 'grey', colour = 'red2')+
  ggtitle("Length of Reporting Gap, by Day")+
  labs(x = "", y = "")+
  bar.theme+
  theme(axis.text.x = element_text(size = 6, angle = 90, vjust = 0))+
  theme(axis.text.y = element_text(size = 7))+
  theme(strip.background = element_rect(fill = 'white'),
        strip.text.x = element_text(size = 6))
```

As you can see, most crimes are reported within a day of occurring. However, crimes that are not reported
immediately (for whatever reason) seem to get lumped on the first day of a month
and at the midnight hour of whatever day they land on. This all seems to be related to using the 'date occurred' 
versus the 'date reported' variable to create my time variables. Several of these 
peaks--hourly and daily-- disappear when you fashion the time variables from the 
'date reported' variable. It is not immediately obvious to me which variable is the 
better choice. I was unable to find out precisely how either date was created(i.e., time report
was filed? time officer was on scene? estimation of when offense took place?) so I will 
stick with the 'date occurred' for now.

What we can gather from all this is two-fold. First, there is some rather significant 
reporting bias occurring depending on what part of the data we look at. The small, but 
significant, reporting gap present in the data makes viewing the data in certain manners
difficult, though not impossible. Ideally, we would have one date, an accurate representation
of when the crime happened (and if you wanted metrics like officer response time that would 
be a separate variable).  Of course, for crimes where the offense takes place over
a period of time--like fraud--this doesn't work too well.  This probably plays into 
why the data are recorded like they are.

Second, even with this reporting bias, we see clear hourly, weekly and monthly periods.
In the next section, we will use this knowledge to attempt some simple forecasting.

# Forecasting Crime Counts
As we have seen from the analysis so far, the overall crime count is composed of several 
elements. There are hourly, weekly, and monthly seasonal elements at the very least. But 
there are also elements that might be considered 'noise', such as the reporting anomolies
we have been discussing so far. Luckily, the field of forecasting is vast and well explored
and we can draw from it to inform our analysis. Much of the following analysis is based on 
Rob Hyndman's excellent open source text [Forecasting: principles and practice](https://www.otexts.org/fpp).

Forecasting typically breaks down a time series into three types of patterns-- trend, seasonal 
and cyclic(details [here](https://www.otexts.org/fpp/6/1). The hourly, weekly, and monthly
variation fall into the seasonal category, defined by their fixed and known periods of fluctuation.
A trend will be a long term rise or fall in the crime counts. Cyclic patterns are similar
to seasonal patterns except their periods are not of fixed length (and the precise length
may not be known). We can then combine these components in the following manner to model
our time series $\ y_{t} = S_{t} + T_{t} + E_{t}$. $\ y_{t}$ is the data at period $\ {t}$,
$\ S_{t}$ is the seasonal component, $\ T_{t}$ is a combination term composed of trend and 
cycle and $\ E_{t}$ is the error term(essentially variation/noise unexplained by the other terms).
This is all vastly simplified, but that is the basic gist of time series decomposition.

## Montly Forecasts

### Naive Baseline Models
The courser the data, the simplier it is to forecast, so we will actually start by looking
at monthly crime counts.  To give us a useful forecast, we setup a simple training and testing
data set, though for a more accurate assessment, we would want to use some form of
cross validation. To get a baseline for model performance, I did several naive forecasting
methods.  The average method simply predicts the historical average for all future values.
The drift method basically extrapolates the future by drawing a line between the first and
last data points. The seasonal naive method takes into account seasonality by forecasting
the value to be equal to the last observed value from the same period the year before.

When you view the results, it is pretty clear that none of the naive forecasts do a very good job, 
although the seasonal naive does look a little closer.  But evaluating the model by RMSE
(all metrics yield similar results with this dataset) we see the drift model gives the
best results.  So our baseline to beat will be the drift model.

```{r naive_monthly_forecasts, echo = FALSE, cache = TRUE, out.width = '1100px', warning = FALSE, message = FALSE}
library(forecast)
full_crime <- create_date_variables(raw_data)
###################################################################################
#Monthly Forecasting
monthly_sums <- full_crime%>%
  filter(year >= 2006)%>%
  group_by(year, month)%>%
  summarise(count = n())

# Creating time series and splitting into train and test sets
monthly_ts <- ts(monthly_sums$count, start = 2006, frequency = 12)
monthly_train <- window(monthly_ts, start = c(2006, 1), end = c(2015,12))
monthly_test <- window(monthly_ts, start = 2016, end = c(2016, 4))

# Creating naive forecasts
m_mean <- meanf(monthly_train, h = 6)
m_drift <- rwf(monthly_train, h = 6, drift = TRUE)
m_sn <- snaive(monthly_train, h = 6)


plot(m_mean, plot.conf = FALSE, main = "Forecasts for Monthly Crime Counts")
lines(m_sn$mean, col = 'red')
lines(m_drift$mean, col = 'green')
lines(monthly_test, lty = 2)
legend("topright", lty = c(1,1, 1, 2), col = c(4, 2, 3, 1),
       legend = c("Mean Method", "Seasonal Naive", "Drift Method", "Actual Values for 2016"))

# Evaluating accuracy of naive models and presenting RMSE in data frame
m_mean_acc <- accuracy(m_mean, monthly_test)
m_sn_acc <- accuracy(m_sn, monthly_test)
m_drift_acc <- accuracy(m_drift, monthly_test)

acc_df <- data.frame(RMSE = c(m_mean_acc[2, 2], m_sn_acc[2, 2], m_drift_acc[2, 2]), row.names = c("Mean", "Seasonal Naive", "Drift"))
acc_df
```

### Exponential Smoothing, ARIMA and Decomposition Models

Now, let's look at some more advanced models. I will not delve into the details of these, 
but I will provide links for those interested in more of the nitty gritty.  The first model
is known as [exponential smoothing](https://www.otexts.org/fpp/7).  It is essentially a weighted average of historical
observations, but the weights are actually decay as they get older.  This means more recent 
observations become more important, which seems very logical for forecasting. I believe these
models were developed to help address trend and seasonality issues in time series, so it 
seems like they should do well with our crime counts.

Second, I fit an [Autoregressive Integrated Moving Average (ARIMA)](https://www.otexts.org/fpp/8) model. 
Unlike exponential smoothing, ARIMA models are looking at historical correlations of the variable of interest with itself
at different periods in the past.  Then, the forecast is created with a linear combination
of these values. 

Finally, the [time series decomposition](https://www.otexts.org/fpp/6/6) model is fit by first decomposing the time series into 
its seasonal(seasonal and cyclic) and nonseasonal(trend and error) components and then separately forecasting 
each component.

```{r monthly_forecasts_cont, echo = FALSE, cache = TRUE, out.width = '1100px', warning = FALSE, message = FALSE}
# Exponential fit
exp_fit <- ets(monthly_train)

plot(forecast(exp_fit, h = 6), monthly_test, main = "Montly Crime Forecast with Exponential Smoothing")
lines(monthly_test, col = 'red')
legend("topright", lty = 1, col = c('blue', 'red'),
       legend = c("Exponential Smoothing", "Actual Values for 2016"))

m_exp_acc <- accuracy(forecast(exp_fit), monthly_test)

# Arima fit
ari_fit <- auto.arima(monthly_train)

plot(forecast(ari_fit, h = 6), monthly_test, main = "Monthly Crime Forecast with ARIMA")
lines(monthly_test, col = 'red')
legend("topright", lty = 1, col = c('blue', 'red'),
       legend = c("ARIMA", "Actual Values for 2016"))

m_ari_acc <- accuracy(forecast(ari_fit), monthly_test)

# Time Series Decomp
m_decomp <- stl(monthly_train, s.window = 7)
m_decomp_fc <- forecast(m_decomp, h = 6)
plot(m_decomp_fc, main = "Montly Crime Forecast with Decomposition")
lines(monthly_test, col = 'red')
legend("topright", lty = 1, col = c('blue', 'red'),
       legend = c("Decomposition", "Actual Values for 2016"))

m_decomp_acc <- accuracy(m_decomp_fc, monthly_test)

monthly_acc_df <- data.frame(RMSE = c(m_mean_acc[2, 2], m_sn_acc[2, 2], m_drift_acc[2, 2], 
                              m_exp_acc[2, 2], m_ari_acc[2, 2], m_decomp_acc[2, 2]),
                     row.names = c("Mean", "Seasonal Naive", "Drift", "Exponential",
                                   "ARIMA", "Decomposition"))
monthly_acc_df
```

TO DO

```{r daily_forecasts, echo = FALSE, cache = TRUE, out.width = '1100px', warning = FALSE, message = FALSE}
####################################################################################
# Daily Forecasting
daily_sums <- full_crime%>%
  filter(year >= 2006)%>%
  group_by(date)%>%
  summarise(count = n())

#creates daily time series with multiple periods; then creates train/test split
daily_ts <- msts(daily_sums$count, start = 2006, seasonal.periods = c(7, 365.25))
daily_train <- window(daily_ts, start = 2006, end = 2015)
daily_test <- window(daily_ts, start = 2015)

# Creating baseline models and assessing accuracy
d_mean <- meanf(daily_train, h = 365)
d_drift <- rwf(daily_train, h = 365)
d_snaive <- snaive(daily_train, h = 365)

d_mean_acc <- accuracy(d_mean, daily_test)
d_drift_acc <- accuracy(d_drift, daily_test)
d_snaive_acc <- accuracy(d_snaive, daily_test)

plot(d_mean, plot.conf = FALSE, main = "Forecasts for Daily Crime Counts")
lines(d_snaive$mean, col = 'red')
lines(d_drift$mean, col = 'green')
# lines(daily_test, lty = 2)
legend("topright", lty = c(1,1, 1, 2), col = c(4, 2, 3, 1),
       legend = c("Mean Method", "Seasonal Naive", "Drift Method", "Actual Values for 2016"))

# Time Series Decomp Forecast
d_decomp <- stl(daily_train, s.window = 7)

decomp_fc <- forecast(d_decomp, h = 365)
plot(decomp_fc, main = "Forecasts for Daily Crime Counts using Decomposition")

d_decomp_acc <- accuracy(decomp_fc, daily_test)



# tbats with weekly, monthly and yearly periods
daily_ts <- msts(daily_sums$count, start = 2006, seasonal.periods = c(7, 30, 365.25))
daily_train <- window(daily_ts, start = 2006, end = 2015)
daily_test <- window(daily_ts, start = 2015)

tbat_fit <- tbats(daily_train)
# plot(tbat_fit)
tbat_fc <- forecast(tbat_fit, h = 365)
plot(tbat_fc, main = "Forecasts for Daily Crime Counts using Tbats")
# lines(daily_test, col = 'red')

d_tbats_acc <- accuracy(tbat_fc, daily_test)

daily_acc_df <- data.frame(RMSE = c(d_mean_acc[2, 2], d_snaive_acc[2, 2], d_drift_acc[2, 2], 
                              d_decomp_acc[2, 2], d_tbats_acc[2, 2]),
                     row.names = c("Mean", "Seasonal Naive", "Drift", "Decomposition",
                                   "TBATS"))
daily_acc_df


# Regression with ARIMA errors


# library(dygraphs)
# 
# # Round dates to the hour for time series grouping
# crime_lou$rounded_date <- round(crime_lou$date_occured, units = 'hours')
# 
# # Create time series counts
# crime_ts <- crime_lou%>%
#   group_by(rounded_date = as.character(rounded_date))%>%
#   summarise(count = as.numeric(n()))
# 
# # There are several hours that are missing which will cause problems creating xts objects
# # It is easy to add those values in with zero values
# time_index <- seq(from = as.POSIXct("2005-01-01 00:00:00"),to = as.POSIXct("2015-12-31 23:59:59"), by ='hour')
# crime_ts <- left_join(data.frame(rounded_date = as.character(time_index)), crime_ts, by = 'rounded_date')
# crime_ts$count[is.na(crime_ts$count)] <- 0
# 
# # Creates an xts object necessary for dygraphs
# xts_ts <- xts::xts(crime_ts$count, order.by = time_index, frequency = 8766)
# 
# dygraph(xts_ts)%>%
#   dyRangeSelector()
```

# Conclusion
TO DO

```{r plot_x_by_hour_function, echo = FALSE, cache = TRUE, warning = FALSE, message = FALSE}
plot_x_by_hour <- function(df, variable){
  df%>%
    group_by_(variable, ~hour)%>%
    summarise_(count = ~n())%>%
    ggplot(aes(x = hour, y = count))+
    geom_bar(stat = 'identity', fill = 'darkblue', alpha = .4)+
    facet_wrap(variable)+
    stat_smooth(method = 'auto', size = .7, alpha = .4, fill = "grey", colour = "red2")+
    ggtitle("Hourly Crime Counts")+
    scale_x_discrete(limits = seq(0, 23, by = 2))+
    labs(x = "Hour", y = "Count")+
    bar.theme+
    theme(axis.text.x = element_text( size = 7))
}
# 
# crime_lou%>%
#   filter(crime_type == 'fraud')%>%
#   group_by(crime_type, reporting)%>%
#   summarise(count = n())%>%
#   ggplot(aes(x = reporting, y = count))+
#   geom_bar(stat = 'identity', fill = 'darkblue', alpha = .4)+
#   facet_wrap(~crime_type)
```



